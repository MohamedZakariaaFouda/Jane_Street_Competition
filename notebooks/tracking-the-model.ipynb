{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":11305158,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport polars as pl\nimport gc\nimport os\nimport joblib \nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost.callback import EarlyStopping\nfrom catboost import CatBoostRegressor\nimport kaggle_evaluation.jane_street_inference_server\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-25T13:50:22.036906Z","iopub.execute_input":"2025-12-25T13:50:22.037195Z","iopub.status.idle":"2025-12-25T13:50:30.117097Z","shell.execute_reply.started":"2025-12-25T13:50:22.037175Z","shell.execute_reply":"2025-12-25T13:50:30.116317Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ============================\n# Reduce Memory Usage Function\n# ============================\ndef reduce_memory_usage(df,float16_as32=False):\n    start_mem = df.memory_usage().sum()/1024**2\n    print(f'df memory usage before reduce : {start_mem} MB')\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # Skip non-numeric columns\n        if col_type.kind not in ['i','f']:\n            continue\n        \n        c_min = df[col].min()\n        c_max = df[col].max()\n\n        # Integer types\n        if col_type.kind in ['i']:\n            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            else:\n                df[col] = df[col].astype(np.int64)\n\n        # Float types\n        else:\n            if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float32 if float16_as32 else np.float16)\n            elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n        \n    end_mem = df.memory_usage().sum()/1024**2\n    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n    print(f\"Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T13:50:30.118203Z","iopub.execute_input":"2025-12-25T13:50:30.118656Z","iopub.status.idle":"2025-12-25T13:50:30.126174Z","shell.execute_reply.started":"2025-12-25T13:50:30.118636Z","shell.execute_reply":"2025-12-25T13:50:30.125456Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# -------------------------------------------\n# Custom Weighted Zero-Mean R² for Lgb Model\n# ------------------------------------------\ndef weighted_zero_mean_r2_lgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n\n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm) ** 2)\n    denominator = np.sum(sample_weight * (y_true_zm) ** 2)\n\n    r2 = 1 - numerator / (denominator + 1e-38)\n    return \"weighted_zero_mean_r2\", r2, True   # maximize=True\n# -------------------------------------------\n# Custom Weighted Zero-Mean R² for Xgb Model\n# -------------------------------------------\ndef weighted_zero_mean_r2_xgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n    \n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm)**2)\n    denominator = np.sum(sample_weight * (y_true_zm)**2)\n    \n    r2 = 1 - numerator / (denominator + 1e-38)\n    return r2        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T13:50:30.126908Z","iopub.execute_input":"2025-12-25T13:50:30.127201Z","iopub.status.idle":"2025-12-25T13:50:30.143337Z","shell.execute_reply.started":"2025-12-25T13:50:30.127174Z","shell.execute_reply":"2025-12-25T13:50:30.142607Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================\n#  Model Dictionary\n# ============================\nmodel_dict = {\n    \"LightGBM\": lambda:LGBMRegressor(\n    n_estimators=3000,          # زيادة عدد الأشجار لتحسين الدقة على التعلم البطيء\n    learning_rate=0.003,        # تقليل LR لتحسين الثبات ونتائج أفضل\n    num_leaves=128,              # عدد أكبر من العقد لزيادة المرونة، مع الحذر من overfitting\n    max_depth=10,               # لتجنب النمو الزائد للشجرة\n    min_child_samples=20,       # الحد الأدنى لعدد العينات في الورقة لتقليل overfitting\n    subsample=0.8,              # لتقليل التباين (bagging)\n    colsample_bytree=0.8,       # لتقليل التباين عن طريق عمود العينات\n    reg_alpha=0.1,              # L1 regularization\n    reg_lambda=0.2,             # L2 regularization\n    random_state=42,\n    max_bin=255,                # زيادة bins قد تحسن الدقة على float16\n    device=\"gpu\"\n    ),\n\n    \"XGBoost\": lambda: XGBRegressor(\n    n_estimators=3000,          # عدد الأشجار لتحسين التعلم البطيء\n    learning_rate=0.003,        # تقليل LR\n    max_depth=8,                # لتجنب overfitting\n    min_child_weight=10,        # لتجنب overfitting\n    subsample=0.8,              # bagging\n    colsample_bytree=0.8,       # column sampling\n    gamma=0.1,                  # regularization على الورقة\n    reg_alpha=0.1,              # L1 regularization\n    reg_lambda=0.2,             # L2 regularization\n    objective=\"reg:squarederror\",\n    tree_method=\"gpu_hist\",      # GPU fast training\n    eval_metric=weighted_zero_mean_r2_xgb,\n    random_state=42\n    ),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T15:17:48.652368Z","iopub.execute_input":"2025-12-25T15:17:48.652643Z","iopub.status.idle":"2025-12-25T15:17:48.658193Z","shell.execute_reply.started":"2025-12-25T15:17:48.652622Z","shell.execute_reply":"2025-12-25T15:17:48.657423Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# use the Kaggle input directory\ntrain_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet'\n\n# Responders_Columns\nfeatures_cols = [f\"feature_{i:02d}\"for i in range(79)]\n\n# Define the target \ntarget = 'responder_6'\n\n#  Create models directory \nos.makedirs(\"models\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T13:50:30.161231Z","iopub.execute_input":"2025-12-25T13:50:30.161478Z","iopub.status.idle":"2025-12-25T13:50:30.174032Z","shell.execute_reply.started":"2025-12-25T13:50:30.161460Z","shell.execute_reply":"2025-12-25T13:50:30.173315Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Preapare Valid_df","metadata":{}},{"cell_type":"code","source":"# prepare valid_df\nskip_dates= 1499  # I will use last 200 days for validation\nvalid_df = pd.read_parquet(train_path, filters=[('date_id','>=', skip_dates)])\nvalid_df = reduce_memory_usage(valid_df)\n\n# X,y,w \nX_valid = valid_df[features_cols + ['time_id']]\ny_valid = valid_df[target]\nw_valid = valid_df[\"weight\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T13:50:30.174977Z","iopub.execute_input":"2025-12-25T13:50:30.175150Z","iopub.status.idle":"2025-12-25T13:50:41.900176Z","shell.execute_reply.started":"2025-12-25T13:50:30.175137Z","shell.execute_reply":"2025-12-25T13:50:41.899337Z"}},"outputs":[{"name":"stdout","text":"df memory usage before reduce : 2510.1318740844727 MB\nMemory usage after optimization is: 1290.52 MB\nDecreased by 48.6%\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Prepare train data & Tracking Train models with mlflow","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install wandb -q\nimport wandb\nwandb.login(key=\"your _wandb_api \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T14:00:20.048043Z","iopub.execute_input":"2025-12-25T14:00:20.048395Z","iopub.status.idle":"2025-12-25T14:00:23.562888Z","shell.execute_reply.started":"2025-12-25T14:00:20.048372Z","shell.execute_reply":"2025-12-25T14:00:23.561982Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# =====================================================\n# W&B Setup for Kaggle (Clean & Correct)\n# =====================================================\n\nimport os\nimport gc\nimport wandb\nimport pandas as pd\nimport joblib\n\n# Create directories\nos.makedirs(\"models\", exist_ok=True)\n\n# =========================\n# Config\n# =========================\nSTART_TRAIN = 1099\nEND_TRAIN   = 1299\nfolds = 2\n\nmodels = []\n\n# =========================================================\n# Training Loop\n# =========================================================\nfor i in range(folds):\n\n    print(f'Load train data and apply reduce memory function on Fold {i+1}')\n\n    # ===== Load Train Data =====\n    train_df = pd.read_parquet(\n        train_path,\n        filters=[[('date_id', '>=', START_TRAIN), ('date_id', '<', END_TRAIN)]]\n    )\n    train_df = reduce_memory_usage(train_df)\n\n    X_train = train_df[features_cols + ['time_id']]\n    y_train = (\n        train_df[target]\n        + 0.5 * train_df['responder_7']\n        + 0.5 * train_df['responder_8']\n    )\n    w_train = train_df[\"weight\"]\n\n    print(f\"\\n================ Fold {i+1}/{folds} ================\")\n    print(\n        f\"Train dates: from day {train_df['date_id'].min()} \"\n        f\"to {train_df['date_id'].max()} \"\n        f\"({train_df['date_id'].nunique()} days)\"\n    )\n\n    # =====================================================\n    # Track X features\n    # =====================================================\n    x_features_path = f\"X_features_fold_{i+1}.txt\"\n    with open(x_features_path, \"w\") as f:\n        for col in X_train.columns:\n            f.write(col + \"\\n\")\n\n    Y_TARGET_EXPR = \"target + 0.5*responder_7 + 0.5*responder_8\"\n\n    # =====================================================\n    # Train Models\n    # =====================================================\n    for model_name, model_class in model_dict.items():\n\n        run_name = f\"{model_name}_Fold_{i+1}_suggest2\"\n\n        model = model_class()\n\n        # ===== W&B Run =====\n        wandb_run = wandb.init(\n            project=\"JS_Kaggle_Experiments1\",\n            entity=\"mohamedzakariaafouda-mansoura-university\",\n            name=run_name,\n            reinit=True,\n            group=f\"window_{START_TRAIN}_{END_TRAIN}\",\n            tags=[model_name,\"suggest\"],\n            config={\n                \"model\": model_name,\n                \"fold\": i + 1,\n                \"n_features\": X_train.shape[1],\n                \"train_window\": f\"{START_TRAIN}_{END_TRAIN}\",\n                \"y_target_expr\": Y_TARGET_EXPR,\n                **model.get_params()\n            }\n        )\n\n        print(f'\\n============== {model_name} | Fold {i+1} ==============')\n\n        # ===== Train =====\n        if model_name == \"LightGBM\":\n\n            model.fit(\n                X_train, y_train,\n                sample_weight=w_train,\n                eval_set=[(X_valid, y_valid)],\n                eval_sample_weight=[w_valid],\n                eval_metric=weighted_zero_mean_r2_lgb,\n                callbacks=[lgb.early_stopping(100)]\n            )\n\n            best_iter = model.best_iteration_\n            best_score = model.best_score_['valid_0']['weighted_zero_mean_r2']\n\n        else:  # XGBoost\n\n            model.fit(\n                X_train, y_train,\n                sample_weight=w_train,\n                eval_set=[(X_valid, y_valid)],\n                sample_weight_eval_set=[w_valid],\n                callbacks=[EarlyStopping(\n                    rounds=100,\n                    maximize=True,\n                    save_best=True\n                )],\n                verbose=20\n            )\n\n            best_iter = model.best_iteration\n            best_score = model.best_score\n\n        print(f\"Best iteration: {best_iter}\")\n        print(f\"Best score: {best_score}\")\n\n        # ===== Log metrics =====\n        wandb.log({\n            \"best_iteration\": best_iter,\n            \"best_score\": best_score\n        })\n\n        # ===== Save model & artifacts =====\n        model_file = f\"models/{model_name}_Fold_{i+1}.model\"\n        joblib.dump(model, model_file)\n\n        wandb.save(model_file)\n        wandb.save(x_features_path)\n\n        models.append((model_name, i + 1, model))\n\n        wandb_run.finish()\n\n        del model\n        gc.collect()\n\n    del train_df, X_train, y_train, w_train\n    gc.collect()\n\n    if folds > 1:\n        START_TRAIN += 200\n        END_TRAIN   += 200\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T15:18:39.080036Z","iopub.execute_input":"2025-12-25T15:18:39.080322Z","iopub.status.idle":"2025-12-25T15:39:33.923942Z","shell.execute_reply.started":"2025-12-25T15:18:39.080299Z","shell.execute_reply":"2025-12-25T15:39:33.923372Z"}},"outputs":[{"name":"stdout","text":"Load train data and apply reduce memory function on Fold 1\ndf memory usage before reduce : 2522.8769760131836 MB\nMemory usage after optimization is: 1297.07 MB\nDecreased by 48.6%\n\n================ Fold 1/2 ================\nTrain dates: from day 1099 to 1298 (200 days)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251225_151848-w3jwq7ws</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/w3jwq7ws' target=\"_blank\">LightGBM_Fold_1_suggest2</a></strong> to <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/w3jwq7ws' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/w3jwq7ws</a>"},"metadata":{}},{"name":"stdout","text":"\n============== LightGBM | Fold 1 ==============\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 19634\n[LightGBM] [Info] Number of data points in the train set: 7472960, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (570.14 MB) transferred to GPU in 0.543681 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.006229\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[237]\tvalid_0's l2: 0.595514\tvalid_0's weighted_zero_mean_r2: 0.00433453\nBest iteration: 237\nBest score: 0.004334531552220078\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>▁</td></tr><tr><td>best_score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>237</td></tr><tr><td>best_score</td><td>0.00433</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">LightGBM_Fold_1_suggest2</strong> at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/w3jwq7ws' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/w3jwq7ws</a><br> View project at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251225_151848-w3jwq7ws/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251225_152428-w9fed6ol</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/w9fed6ol' target=\"_blank\">XGBoost_Fold_1_suggest2</a></strong> to <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/w9fed6ol' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/w9fed6ol</a>"},"metadata":{}},{"name":"stdout","text":"\n============== XGBoost | Fold 1 ==============\n[0]\tvalidation_0-rmse:0.77335\tvalidation_0-weighted_zero_mean_r2_xgb:0.00006\n[20]\tvalidation_0-rmse:0.77296\tvalidation_0-weighted_zero_mean_r2_xgb:0.00108\n[40]\tvalidation_0-rmse:0.77264\tvalidation_0-weighted_zero_mean_r2_xgb:0.00191\n[60]\tvalidation_0-rmse:0.77241\tvalidation_0-weighted_zero_mean_r2_xgb:0.00251\n[80]\tvalidation_0-rmse:0.77221\tvalidation_0-weighted_zero_mean_r2_xgb:0.00302\n[100]\tvalidation_0-rmse:0.77208\tvalidation_0-weighted_zero_mean_r2_xgb:0.00335\n[120]\tvalidation_0-rmse:0.77200\tvalidation_0-weighted_zero_mean_r2_xgb:0.00354\n[140]\tvalidation_0-rmse:0.77195\tvalidation_0-weighted_zero_mean_r2_xgb:0.00368\n[160]\tvalidation_0-rmse:0.77190\tvalidation_0-weighted_zero_mean_r2_xgb:0.00379\n[180]\tvalidation_0-rmse:0.77190\tvalidation_0-weighted_zero_mean_r2_xgb:0.00379\n[200]\tvalidation_0-rmse:0.77192\tvalidation_0-weighted_zero_mean_r2_xgb:0.00376\n[220]\tvalidation_0-rmse:0.77192\tvalidation_0-weighted_zero_mean_r2_xgb:0.00375\n[240]\tvalidation_0-rmse:0.77200\tvalidation_0-weighted_zero_mean_r2_xgb:0.00356\n[260]\tvalidation_0-rmse:0.77205\tvalidation_0-weighted_zero_mean_r2_xgb:0.00340\n[275]\tvalidation_0-rmse:0.77210\tvalidation_0-weighted_zero_mean_r2_xgb:0.00329\nBest iteration: 176\nBest score: 0.003816\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>▁</td></tr><tr><td>best_score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>176</td></tr><tr><td>best_score</td><td>0.00382</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">XGBoost_Fold_1_suggest2</strong> at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/w9fed6ol' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/w9fed6ol</a><br> View project at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251225_152428-w9fed6ol/logs</code>"},"metadata":{}},{"name":"stdout","text":"Load train data and apply reduce memory function on Fold 2\ndf memory usage before reduce : 2454.9030990600586 MB\nMemory usage after optimization is: 1262.13 MB\nDecreased by 48.6%\n\n================ Fold 2/2 ================\nTrain dates: from day 1299 to 1498 (200 days)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251225_152750-kucg2ylv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/kucg2ylv' target=\"_blank\">LightGBM_Fold_2_suggest2</a></strong> to <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/kucg2ylv' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/kucg2ylv</a>"},"metadata":{}},{"name":"stdout","text":"\n============== LightGBM | Fold 2 ==============\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 19637\n[LightGBM] [Info] Number of data points in the train set: 7271616, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (554.78 MB) transferred to GPU in 0.517257 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.003815\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[397]\tvalid_0's l2: 0.593736\tvalid_0's weighted_zero_mean_r2: 0.00719065\nBest iteration: 397\nBest score: 0.007190647703161468\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>▁</td></tr><tr><td>best_score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>397</td></tr><tr><td>best_score</td><td>0.00719</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">LightGBM_Fold_2_suggest2</strong> at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/kucg2ylv' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/kucg2ylv</a><br> View project at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251225_152750-kucg2ylv/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251225_153523-vr4jgk5t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/vr4jgk5t' target=\"_blank\">XGBoost_Fold_2_suggest2</a></strong> to <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/vr4jgk5t' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/vr4jgk5t</a>"},"metadata":{}},{"name":"stdout","text":"\n============== XGBoost | Fold 2 ==============\n[0]\tvalidation_0-rmse:0.77332\tvalidation_0-weighted_zero_mean_r2_xgb:0.00006\n[20]\tvalidation_0-rmse:0.77290\tvalidation_0-weighted_zero_mean_r2_xgb:0.00114\n[40]\tvalidation_0-rmse:0.77254\tvalidation_0-weighted_zero_mean_r2_xgb:0.00207\n[60]\tvalidation_0-rmse:0.77222\tvalidation_0-weighted_zero_mean_r2_xgb:0.00289\n[80]\tvalidation_0-rmse:0.77195\tvalidation_0-weighted_zero_mean_r2_xgb:0.00358\n[100]\tvalidation_0-rmse:0.77172\tvalidation_0-weighted_zero_mean_r2_xgb:0.00418\n[120]\tvalidation_0-rmse:0.77151\tvalidation_0-weighted_zero_mean_r2_xgb:0.00471\n[140]\tvalidation_0-rmse:0.77133\tvalidation_0-weighted_zero_mean_r2_xgb:0.00518\n[160]\tvalidation_0-rmse:0.77118\tvalidation_0-weighted_zero_mean_r2_xgb:0.00557\n[180]\tvalidation_0-rmse:0.77104\tvalidation_0-weighted_zero_mean_r2_xgb:0.00590\n[200]\tvalidation_0-rmse:0.77093\tvalidation_0-weighted_zero_mean_r2_xgb:0.00620\n[220]\tvalidation_0-rmse:0.77083\tvalidation_0-weighted_zero_mean_r2_xgb:0.00644\n[240]\tvalidation_0-rmse:0.77076\tvalidation_0-weighted_zero_mean_r2_xgb:0.00664\n[260]\tvalidation_0-rmse:0.77069\tvalidation_0-weighted_zero_mean_r2_xgb:0.00681\n[280]\tvalidation_0-rmse:0.77064\tvalidation_0-weighted_zero_mean_r2_xgb:0.00694\n[300]\tvalidation_0-rmse:0.77059\tvalidation_0-weighted_zero_mean_r2_xgb:0.00707\n[320]\tvalidation_0-rmse:0.77056\tvalidation_0-weighted_zero_mean_r2_xgb:0.00715\n[340]\tvalidation_0-rmse:0.77053\tvalidation_0-weighted_zero_mean_r2_xgb:0.00722\n[360]\tvalidation_0-rmse:0.77051\tvalidation_0-weighted_zero_mean_r2_xgb:0.00726\n[380]\tvalidation_0-rmse:0.77051\tvalidation_0-weighted_zero_mean_r2_xgb:0.00728\n[400]\tvalidation_0-rmse:0.77051\tvalidation_0-weighted_zero_mean_r2_xgb:0.00728\n[420]\tvalidation_0-rmse:0.77051\tvalidation_0-weighted_zero_mean_r2_xgb:0.00727\n[440]\tvalidation_0-rmse:0.77052\tvalidation_0-weighted_zero_mean_r2_xgb:0.00725\n[460]\tvalidation_0-rmse:0.77053\tvalidation_0-weighted_zero_mean_r2_xgb:0.00721\n[477]\tvalidation_0-rmse:0.77055\tvalidation_0-weighted_zero_mean_r2_xgb:0.00718\nBest iteration: 378\nBest score: 0.007282\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>▁</td></tr><tr><td>best_score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>378</td></tr><tr><td>best_score</td><td>0.00728</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">XGBoost_Fold_2_suggest2</strong> at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/vr4jgk5t' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1/runs/vr4jgk5t</a><br> View project at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Experiments1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251225_153523-vr4jgk5t/logs</code>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}