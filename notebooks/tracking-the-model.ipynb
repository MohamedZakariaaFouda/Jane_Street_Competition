{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":11305158,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport polars as pl\nimport gc\nimport os\nimport joblib \nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost.callback import EarlyStopping\nimport kaggle_evaluation.jane_street_inference_server\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:50:27.521276Z","iopub.execute_input":"2025-12-28T14:50:27.521970Z","iopub.status.idle":"2025-12-28T14:50:29.889832Z","shell.execute_reply.started":"2025-12-28T14:50:27.521946Z","shell.execute_reply":"2025-12-28T14:50:29.889033Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ============================\n# Reduce Memory Usage Function\n# ============================\ndef reduce_memory_usage(df,float16_as32=False):\n    start_mem = df.memory_usage().sum()/1024**2\n    print(f'df memory usage before reduce : {start_mem} MB')\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # Skip non-numeric columns\n        if col_type.kind not in ['i','f']:\n            continue\n        \n        c_min = df[col].min()\n        c_max = df[col].max()\n\n        # Integer types\n        if col_type.kind in ['i']:\n            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            else:\n                df[col] = df[col].astype(np.int64)\n\n        # Float types\n        else:\n            if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float32 if float16_as32 else np.float16)\n            elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n        \n    end_mem = df.memory_usage().sum()/1024**2\n    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n    print(f\"Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:50:32.972119Z","iopub.execute_input":"2025-12-28T14:50:32.972673Z","iopub.status.idle":"2025-12-28T14:50:32.981332Z","shell.execute_reply.started":"2025-12-28T14:50:32.972649Z","shell.execute_reply":"2025-12-28T14:50:32.980388Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# -------------------------------------------\n# Custom Weighted Zero-Mean R² for Lgb Model\n# ------------------------------------------\ndef weighted_zero_mean_r2_lgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n\n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm) ** 2)\n    denominator = np.sum(sample_weight * (y_true_zm) ** 2)\n\n    r2 = 1 - numerator / (denominator + 1e-38)\n    return \"weighted_zero_mean_r2\", r2, True   # maximize=True\n# -------------------------------------------\n# Custom Weighted Zero-Mean R² for Xgb Model\n# -------------------------------------------\ndef weighted_zero_mean_r2_xgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n    \n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm)**2)\n    denominator = np.sum(sample_weight * (y_true_zm)**2)\n    \n    r2 = 1 - numerator / (denominator + 1e-38)\n    return r2        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:50:35.686275Z","iopub.execute_input":"2025-12-28T14:50:35.686906Z","iopub.status.idle":"2025-12-28T14:50:35.692370Z","shell.execute_reply.started":"2025-12-28T14:50:35.686881Z","shell.execute_reply":"2025-12-28T14:50:35.691550Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# use the Kaggle input directory\ntrain_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet'\n\n# Responders_Columns\nfeatures_cols = [f\"feature_{i:02d}\"for i in range(79)]\n\n# Define the target \ntarget = 'responder_6'\n\n#  Create models directory \nos.makedirs(\"models\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:50:38.737017Z","iopub.execute_input":"2025-12-28T14:50:38.737307Z","iopub.status.idle":"2025-12-28T14:50:38.741525Z","shell.execute_reply.started":"2025-12-28T14:50:38.737283Z","shell.execute_reply":"2025-12-28T14:50:38.740743Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Preapare Valid_df","metadata":{}},{"cell_type":"code","source":"# prepare valid_df\nskip_dates= 1499  # I will use last 100 days for validation\nvalid_df = pd.read_parquet(train_path, filters=[('date_id','>=', skip_dates)])\nvalid_df = reduce_memory_usage(valid_df)\n\n# X,y,w \nX_valid = valid_df[features_cols + ['time_id']]\ny_valid = valid_df[target]\nw_valid = valid_df[\"weight\"]\nprint(f\"valid dates: from day {valid_df['date_id'].min()} \"\n        f\"to {valid_df['date_id'].max()} \"\n        f\"({valid_df['date_id'].nunique()} days)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:50:41.893041Z","iopub.execute_input":"2025-12-28T14:50:41.893340Z","iopub.status.idle":"2025-12-28T14:50:47.417854Z","shell.execute_reply.started":"2025-12-28T14:50:41.893311Z","shell.execute_reply":"2025-12-28T14:50:47.417106Z"}},"outputs":[{"name":"stdout","text":"df memory usage before reduce : 1254.5759353637695 MB\nMemory usage after optimization is: 645.01 MB\nDecreased by 48.6%\nvalid dates: from day 1599 to 1698 (100 days)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Prepare train data & Tracking Train models with mlflow","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install wandb -q\nimport wandb\nwandb.login(key=\"e04a3ac3e8b68f363d1fecde23ad2a89bb7d146d\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:50:50.976969Z","iopub.execute_input":"2025-12-28T14:50:50.977690Z","iopub.status.idle":"2025-12-28T14:51:02.393270Z","shell.execute_reply.started":"2025-12-28T14:50:50.977657Z","shell.execute_reply":"2025-12-28T14:51:02.392670Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# =========================\n# Config\n# =========================\nSTART_TRAIN = 1099\nEND_TRAIN   = START_TRAIN + 200\nfolds = 2\nmodel_name = \"Xgboost\"\n\nmodels = []\n\n\n# =========================================================\n# Training Loop\n# =========================================================\nfor i in range(folds):\n\n    print(f'Load train data and apply reduce memory function on Fold {i+1}')\n\n    # ===== Load Train Data =====\n    train_df = pd.read_parquet(\n        train_path,\n        filters=[[('date_id', '>=', START_TRAIN), ('date_id', '<', END_TRAIN)]]\n    )\n    train_df = reduce_memory_usage(train_df)\n\n    X_train = train_df[features_cols + ['time_id']]\n    y_train = (\n        train_df[target]\n        + 0.5 * train_df['responder_7']\n        + 0.5 * train_df['responder_8']\n    )\n    w_train = train_df[\"weight\"]\n\n    print(f\"\\n================ Fold {i+1}/{folds} ================\")\n    print(\n        f\"Train dates: from day {train_df['date_id'].min()} \"\n        f\"to {train_df['date_id'].max()} \"\n        f\"({train_df['date_id'].nunique()} days)\"\n    )\n\n    # =====================================================\n    # Track X features and y_target\n    # =====================================================\n    x_features_path = f\"X_features_fold_{i+1}.txt\"\n    with open(x_features_path, \"w\") as f:\n        for col in X_train.columns:\n            f.write(col + \"\\n\")\n\n    Y_TARGET_EXPR = \"target + 0.5*responder_7 + 0.5*responder_8\"\n\n    \n    # =====================================================\n    # Xgboost Model\n    # =====================================================\n    print(f'\\n============== {model_name} | Fold {i+1} ==============')\n    \n    model = XGBRegressor(\n        n_estimators=2000,\n        learning_rate=0.01,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective=\"reg:squarederror\",\n        device=\"cuda\",\n        tree_method=\"gpu_hist\",\n        max_bin=128,\n        random_state=42,\n        eval_metric=weighted_zero_mean_r2_xgb,\n        disable_default_eval_metric=True,\n\n    )\n\n    model.fit(\n    X_train, y_train,\n    sample_weight=w_train,\n    eval_set=[(X_valid, y_valid)],\n    sample_weight_eval_set=[w_valid],\n    callbacks=[EarlyStopping(rounds=100,maximize=True,save_best=True)],\n    verbose=20\n    )\n\n\n    # =====================================================\n    # Results\n    # =====================================================\n    best_iter = model.best_iteration\n    best_score = model.best_score\n\n    print(f\"Best iteration: {best_iter}\")\n    print(f\"Best score: {best_score}\")\n\n    model_file = f\"models/{model_name}_Fold_{i+1}.model\"\n    joblib.dump(model, model_file)\n    models.append((model_name, i + 1, model))\n\n\n\n    # =====================================================\n    # W&B Run\n    # =====================================================\n    run_name = f\"{model_name}_Fold_{i+1}/{folds}\"\n\n    wandb_run = wandb.init(\n        project=\"JS_Kaggle_Track_Xgboost\",\n        entity=\"mohamedzakariaafouda-mansoura-university\",\n        name=run_name,\n        reinit=True,\n        group=f\"window_{START_TRAIN}_{END_TRAIN}\",\n        tags=[\"Xgboost\",\"4folds\"],\n        config={\n            \"fold\": i + 1,\n            \"n_features\": X_train.shape[1],\n            \"train_window\": f\"{START_TRAIN}_{END_TRAIN}\",\n            \"Number of days\": f\"({train_df['date_id'].nunique()} days)\",\n            \"y_target_expr\": Y_TARGET_EXPR,\n            **model.get_params()\n        }\n    )\n\n\n    wandb.log({\n        \"best_iteration\": best_iter,\n        \"best_score\": best_score\n    })\n\n    # =====================================================\n    # Save artifacts\n    # =====================================================\n    \n    wandb.save(model_file)\n    wandb.save(x_features_path)\n    wandb_run.finish()\n\n    \n    del model\n    del train_df, X_train, y_train, w_train\n    gc.collect()\n\n    if folds > 1:\n        START_TRAIN += 200\n        END_TRAIN   = START_TRAIN + 200\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:58:07.676494Z","iopub.execute_input":"2025-12-28T14:58:07.677084Z","iopub.status.idle":"2025-12-28T15:04:56.337903Z","shell.execute_reply.started":"2025-12-28T14:58:07.677059Z","shell.execute_reply":"2025-12-28T15:04:56.337275Z"}},"outputs":[{"name":"stdout","text":"Load train data and apply reduce memory function on Fold 1\ndf memory usage before reduce : 2448.040351867676 MB\nMemory usage after optimization is: 1258.60 MB\nDecreased by 48.6%\n\n================ Fold 1/3 ================\nTrain dates: from day 999 to 1198 (200 days)\n\n============== Xgboost | Fold 1 ==============\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00015\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00235\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00332\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00374\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00386\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00375\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00351\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00322\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00298\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00268\n[183]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00264\nBest iteration: 83\nBest score: 0.00389\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251228_150014-d1nzkps3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/d1nzkps3' target=\"_blank\">Xgboost_Fold_1/3</a></strong> to <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/d1nzkps3' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/d1nzkps3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>▁</td></tr><tr><td>best_score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>83</td></tr><tr><td>best_score</td><td>0.00389</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">Xgboost_Fold_1/3</strong> at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/d1nzkps3' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/d1nzkps3</a><br> View project at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251228_150014-d1nzkps3/logs</code>"},"metadata":{}},{"name":"stdout","text":"Load train data and apply reduce memory function on Fold 2\ndf memory usage before reduce : 2495.4259872436523 MB\nMemory usage after optimization is: 1282.96 MB\nDecreased by 48.6%\n\n================ Fold 2/3 ================\nTrain dates: from day 1199 to 1398 (200 days)\n\n============== Xgboost | Fold 2 ==============\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00013\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00207\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00317\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00379\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00409\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00411\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00398\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00372\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00354\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00324\n[189]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00312\nBest iteration: 90\nBest score: 0.004162\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251228_150229-a7eezrwe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/a7eezrwe' target=\"_blank\">Xgboost_Fold_2/3</a></strong> to <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/a7eezrwe' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/a7eezrwe</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>▁</td></tr><tr><td>best_score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>90</td></tr><tr><td>best_score</td><td>0.00416</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">Xgboost_Fold_2/3</strong> at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/a7eezrwe' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/a7eezrwe</a><br> View project at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251228_150229-a7eezrwe/logs</code>"},"metadata":{}},{"name":"stdout","text":"Load train data and apply reduce memory function on Fold 3\ndf memory usage before reduce : 2468.628593444824 MB\nMemory usage after optimization is: 1269.18 MB\nDecreased by 48.6%\n\n================ Fold 3/3 ================\nTrain dates: from day 1399 to 1598 (200 days)\n\n============== Xgboost | Fold 3 ==============\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00012\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00212\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00336\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00413\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00460\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00494\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00512\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00512\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00503\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00487\n[200]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00468\n[218]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00445\nBest iteration: 119\nBest score: 0.005133\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251228_150448-04bj29do</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/04bj29do' target=\"_blank\">Xgboost_Fold_3/3</a></strong> to <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/04bj29do' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/04bj29do</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>▁</td></tr><tr><td>best_score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>119</td></tr><tr><td>best_score</td><td>0.00513</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">Xgboost_Fold_3/3</strong> at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/04bj29do' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost/runs/04bj29do</a><br> View project at: <a href='https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost' target=\"_blank\">https://wandb.ai/mohamedzakariaafouda-mansoura-university/JS_Kaggle_Track_Xgboost</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251228_150448-04bj29do/logs</code>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"del train_df, X_train, y_train, w_train\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T14:54:36.207884Z","iopub.execute_input":"2025-12-28T14:54:36.208187Z","iopub.status.idle":"2025-12-28T14:54:36.413423Z","shell.execute_reply.started":"2025-12-28T14:54:36.208167Z","shell.execute_reply":"2025-12-28T14:54:36.412652Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"668"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# ============================\n#  Model Dictionary\n# ============================\nmodel_dict = {\n    \"LightGBM\": lambda:LGBMRegressor(\n    n_estimators=3000,          # زيادة عدد الأشجار لتحسين الدقة على التعلم البطيء\n    learning_rate=0.003,        # تقليل LR لتحسين الثبات ونتائج أفضل\n    num_leaves=128,              # عدد أكبر من العقد لزيادة المرونة، مع الحذر من overfitting\n    max_depth=10,               # لتجنب النمو الزائد للشجرة\n    min_child_samples=20,       # الحد الأدنى لعدد العينات في الورقة لتقليل overfitting\n    subsample=0.8,              # لتقليل التباين (bagging)\n    colsample_bytree=0.8,       # لتقليل التباين عن طريق عمود العينات\n    reg_alpha=0.1,              # L1 regularization\n    reg_lambda=0.2,             # L2 regularization\n    random_state=42,\n    max_bin=255,                # زيادة bins قد تحسن الدقة على float16\n    device=\"gpu\"\n    ),\n\n    \"XGBoost\": lambda: XGBRegressor(\n    n_estimators=3000,          # عدد الأشجار لتحسين التعلم البطيء\n    learning_rate=0.003,        # تقليل LR\n    max_depth=8,                # لتجنب overfitting\n    min_child_weight=10,        # لتجنب overfitting\n    subsample=0.8,              # bagging\n    colsample_bytree=0.8,       # column sampling\n    gamma=0.1,                  # regularization على الورقة\n    reg_alpha=0.1,              # L1 regularization\n    reg_lambda=0.2,             # L2 regularization\n    objective=\"reg:squarederror\",\n    tree_method=\"gpu_hist\",      # GPU fast training\n    eval_metric=weighted_zero_mean_r2_xgb,\n    random_state=42\n    ),\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Config\n# =========================\nSTART_TRAIN = 1099\nEND_TRAIN   = 1299\nfolds = 2\n\nmodels = []\n\n# =========================================================\n# Training Loop\n# =========================================================\nfor i in range(folds):\n\n    print(f'Load train data and apply reduce memory function on Fold {i+1}')\n\n    # ===== Load Train Data =====\n    train_df = pd.read_parquet(\n        train_path,\n        filters=[[('date_id', '>=', START_TRAIN), ('date_id', '<', END_TRAIN)]]\n    )\n    train_df = reduce_memory_usage(train_df)\n\n    X_train = train_df[features_cols + ['time_id']]\n    y_train = (\n        train_df[target]\n        + 0.5 * train_df['responder_7']\n        + 0.5 * train_df['responder_8']\n    )\n    w_train = train_df[\"weight\"]\n\n    print(f\"\\n================ Fold {i+1}/{folds} ================\")\n    print(\n        f\"Train dates: from day {train_df['date_id'].min()} \"\n        f\"to {train_df['date_id'].max()} \"\n        f\"({train_df['date_id'].nunique()} days)\"\n    )\n\n    # =====================================================\n    # Track X features\n    # =====================================================\n    x_features_path = f\"X_features_fold_{i+1}.txt\"\n    with open(x_features_path, \"w\") as f:\n        for col in X_train.columns:\n            f.write(col + \"\\n\")\n\n    Y_TARGET_EXPR = \"target + 0.5*responder_7 + 0.5*responder_8\"\n\n    # =====================================================\n    # Train Models\n    # =====================================================\n    for model_name, model_class in model_dict.items():\n\n        run_name = f\"{model_name}_Fold_{i+1}_suggest2\"\n\n        model = model_class()\n\n        # ===== W&B Run =====\n        wandb_run = wandb.init(\n            project=\"JS_Kaggle_Experiments1\",\n            entity=\"mohamedzakariaafouda-mansoura-university\",\n            name=run_name,\n            reinit=True,\n            group=f\"window_{START_TRAIN}_{END_TRAIN}\",\n            tags=[model_name,\"suggest\"],\n            config={\n                \"model\": model_name,\n                \"fold\": i + 1,\n                \"n_features\": X_train.shape[1],\n                \"train_window\": f\"{START_TRAIN}_{END_TRAIN}\",\n                \"y_target_expr\": Y_TARGET_EXPR,\n                **model.get_params()\n            }\n        )\n\n        print(f'\\n============== {model_name} | Fold {i+1} ==============')\n\n        # ===== Train =====\n        if model_name == \"LightGBM\":\n\n            model.fit(\n                X_train, y_train,\n                sample_weight=w_train,\n                eval_set=[(X_valid, y_valid)],\n                eval_sample_weight=[w_valid],\n                eval_metric=weighted_zero_mean_r2_lgb,\n                callbacks=[lgb.early_stopping(100)]\n            )\n\n            best_iter = model.best_iteration_\n            best_score = model.best_score_['valid_0']['weighted_zero_mean_r2']\n\n        else:  # XGBoost\n\n            model.fit(\n                X_train, y_train,\n                sample_weight=w_train,\n                eval_set=[(X_valid, y_valid)],\n                sample_weight_eval_set=[w_valid],\n                callbacks=[EarlyStopping(\n                    rounds=100,\n                    maximize=True,\n                    save_best=True\n                )],\n                verbose=20\n            )\n\n            best_iter = model.best_iteration\n            best_score = model.best_score\n\n        print(f\"Best iteration: {best_iter}\")\n        print(f\"Best score: {best_score}\")\n\n        # ===== Log metrics =====\n        wandb.log({\n            \"best_iteration\": best_iter,\n            \"best_score\": best_score\n        })\n\n        # ===== Save model & artifacts =====\n        model_file = f\"models/{model_name}_Fold_{i+1}.model\"\n        joblib.dump(model, model_file)\n\n        wandb.save(model_file)\n        wandb.save(x_features_path)\n\n        models.append((model_name, i + 1, model))\n\n        wandb_run.finish()\n\n        del model\n        gc.collect()\n\n    del train_df, X_train, y_train, w_train\n    gc.collect()\n\n    if folds > 1:\n        START_TRAIN += 200\n        END_TRAIN   += 200\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}