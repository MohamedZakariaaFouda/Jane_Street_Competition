{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":11305158,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport polars as pl\nimport gc\nimport os\nimport joblib \nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost.callback import EarlyStopping\nfrom catboost import CatBoostRegressor\nimport kaggle_evaluation.jane_street_inference_server\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-24T17:47:49.453100Z","iopub.execute_input":"2025-12-24T17:47:49.453849Z","iopub.status.idle":"2025-12-24T17:47:55.769965Z","shell.execute_reply.started":"2025-12-24T17:47:49.453824Z","shell.execute_reply":"2025-12-24T17:47:55.769155Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ============================\n# Reduce Memory Usage Function\n# ============================\ndef reduce_memory_usage(df,float16_as32=False):\n    start_mem = df.memory_usage().sum()/1024**2\n    print(f'df memory usage before reduce : {start_mem} MB')\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # Skip non-numeric columns\n        if col_type.kind not in ['i','f']:\n            continue\n        \n        c_min = df[col].min()\n        c_max = df[col].max()\n\n        # Integer types\n        if col_type.kind in ['i']:\n            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            else:\n                df[col] = df[col].astype(np.int64)\n\n        # Float types\n        else:\n            if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float32 if float16_as32 else np.float16)\n            elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n        \n    end_mem = df.memory_usage().sum()/1024**2\n    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n    print(f\"Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T17:48:00.521018Z","iopub.execute_input":"2025-12-24T17:48:00.521632Z","iopub.status.idle":"2025-12-24T17:48:00.529818Z","shell.execute_reply.started":"2025-12-24T17:48:00.521606Z","shell.execute_reply":"2025-12-24T17:48:00.529027Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# -------------------------------------------\n# Custom Weighted Zero-Mean RÂ² for Lgb Model\n# ------------------------------------------\ndef weighted_zero_mean_r2_lgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n\n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm) ** 2)\n    denominator = np.sum(sample_weight * (y_true_zm) ** 2)\n\n    r2 = 1 - numerator / (denominator + 1e-38)\n    return \"weighted_zero_mean_r2\", r2, True   # maximize=True\n# -------------------------------------------\n# Custom Weighted Zero-Mean RÂ² for Xgb Model\n# -------------------------------------------\ndef weighted_zero_mean_r2_xgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n    \n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm)**2)\n    denominator = np.sum(sample_weight * (y_true_zm)**2)\n    \n    r2 = 1 - numerator / (denominator + 1e-38)\n    return r2        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T17:48:04.013464Z","iopub.execute_input":"2025-12-24T17:48:04.014150Z","iopub.status.idle":"2025-12-24T17:48:04.019595Z","shell.execute_reply.started":"2025-12-24T17:48:04.014128Z","shell.execute_reply":"2025-12-24T17:48:04.018828Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================\n#  Model Dictionary\n# ============================\nmodel_dict = {\n    \"LightGBM\": lambda:LGBMRegressor(\n    n_estimators=2000,\n    learning_rate=0.01,\n    num_leaves=50,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    max_bin=128,\n    device=\"gpu\"\n    ),\n\n    \"XGBoost\": lambda: XGBRegressor(\n    n_estimators=2000,\n    learning_rate=0.01,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"reg:squarederror\",\n    device=\"cuda\",\n    tree_method=\"gpu_hist\",\n    max_bin=128,\n    random_state=42,\n    eval_metric=weighted_zero_mean_r2_xgb,\n    disable_default_eval_metric=True\n    ),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T17:48:06.832947Z","iopub.execute_input":"2025-12-24T17:48:06.833195Z","iopub.status.idle":"2025-12-24T17:48:06.838061Z","shell.execute_reply.started":"2025-12-24T17:48:06.833176Z","shell.execute_reply":"2025-12-24T17:48:06.837205Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# use the Kaggle input directory\ntrain_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet'\n\n# Responders_Columns\nfeatures_cols = [f\"feature_{i:02d}\"for i in range(79)]\n\n# Define the target \ntarget = 'responder_6'\n\n#  Create models directory \nos.makedirs(\"models\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T17:48:09.680924Z","iopub.execute_input":"2025-12-24T17:48:09.681198Z","iopub.status.idle":"2025-12-24T17:48:09.685736Z","shell.execute_reply.started":"2025-12-24T17:48:09.681179Z","shell.execute_reply":"2025-12-24T17:48:09.684951Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Preapare Valid_df","metadata":{}},{"cell_type":"code","source":"# prepare valid_df\nskip_dates= 1499  # I will use last 200 days for validation\nvalid_df = pd.read_parquet(train_path, filters=[('date_id','>=', skip_dates)])\nvalid_df = reduce_memory_usage(valid_df)\n\n# X,y,w \nX_valid = valid_df[features_cols + ['time_id']]\ny_valid = valid_df[target]\nw_valid = valid_df[\"weight\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T17:48:14.032970Z","iopub.execute_input":"2025-12-24T17:48:14.033255Z","iopub.status.idle":"2025-12-24T17:48:25.872890Z","shell.execute_reply.started":"2025-12-24T17:48:14.033234Z","shell.execute_reply":"2025-12-24T17:48:25.872265Z"}},"outputs":[{"name":"stdout","text":"df memory usage before reduce : 2510.1318740844727 MB\nMemory usage after optimization is: 1290.52 MB\nDecreased by 48.6%\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Prepare train data & Tracking Train models with mlflow","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install mlflow\nimport mlflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T18:33:49.903826Z","iopub.execute_input":"2025-12-24T18:33:49.904433Z","iopub.status.idle":"2025-12-24T18:33:53.651369Z","shell.execute_reply.started":"2025-12-24T18:33:49.904410Z","shell.execute_reply":"2025-12-24T18:33:53.650549Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# =====================================================\n# MLflow Offline Setup (Kaggle) \n# =====================================================\n\nMLFLOW_DIR = \"/kaggle/working/mlruns\"\nos.makedirs(MLFLOW_DIR, exist_ok=True)\n\nmlflow.set_tracking_uri(f\"file://{MLFLOW_DIR}\")\nmlflow.set_experiment(\"JS_Kaggle_Experiments1\")\n\n# =========================\n# Config \n# =========================\n\nSTART_TRAIN = 1099\nEND_TRAIN   = 1299\nfolds = 2\n\nmodels = []\n\n# =========================================================\n# Training Loop\n# =========================================================\n\nfor i in range(folds):\n    print(f'Load train data and apply reduce memory function on Fold {i+1}')\n\n    # ===== Load Train Data =====\n    train_df = pd.read_parquet(train_path,filters=[[('date_id', '>=', START_TRAIN),('date_id', '<', END_TRAIN)]])\n    train_df = reduce_memory_usage(train_df)\n\n    X_train = train_df[features_cols + ['time_id']]\n    y_train = (train_df[target]+ 0.5 * train_df['responder_7']+ 0.5 * train_df['responder_8'])\n    w_train = train_df[\"weight\"]\n\n    print(f\"\\n================ Fold {i+1}/{folds} ================\")\n    print(f\"Train dates: from day {train_df['date_id'].min()} \"f\"to {train_df['date_id'].max()} \"f\"({train_df['date_id'].nunique()} days)\")\n\n    # =====================================================\n    # Track X features and y target\n    # =====================================================\n    \n    x_features_path = f\"X_features_fold_{i+1}.txt\"\n    with open(x_features_path, \"w\") as f:\n        for col in X_train.columns:\n            f.write(col + \"\\n\")\n\n    Y_TARGET_EXPR = \"target + 0.5*responder_7 + 0.5*responder_8\"\n\n\n    # =====================================================\n    # Train Models\n    # =====================================================\n    for model_name, model_class in model_dict.items():\n\n        run_name = f\"{model_name}_Fold_{i+1}\"\n\n        with mlflow.start_run(run_name=run_name):\n\n            model = model_class()\n\n            # ===== MLflow Params =====\n            mlflow.log_param(\"model\", model_name)\n            mlflow.log_param(\"fold\", i + 1)\n            mlflow.log_param(\"n_features\", X_train.shape[1])\n            mlflow.log_param(\"train_window\", f\"{START_TRAIN}_{END_TRAIN}\")\n            mlflow.log_param(\"y_target_expr\", Y_TARGET_EXPR)\n            mlflow.log_params(model.get_params())\n\n            # ===== Log Artifacts =====\n            mlflow.log_artifact(x_features_path)\n\n            print(f'\\n============== {model_name} | Fold {i+1} ==============')\n\n            # ===== Train =====\n            if model_name == \"LightGBM\":\n\n                model.fit(\n                    X_train, y_train,\n                    sample_weight=w_train,\n                    eval_set=[(X_valid, y_valid)],\n                    eval_sample_weight=[w_valid],\n                    eval_metric=weighted_zero_mean_r2_lgb,\n                    callbacks=[lgb.early_stopping(100)]\n                )\n\n                best_iter = model.best_iteration_\n                best_score = model.best_score_['valid_0']['weighted_zero_mean_r2']\n\n            else:  # XGBoost\n\n                model.fit(\n                    X_train, y_train,\n                    sample_weight=w_train,\n                    eval_set=[(X_valid, y_valid)],\n                    sample_weight_eval_set=[w_valid],\n                    callbacks=[EarlyStopping(\n                        rounds=100,\n                        maximize=True,\n                        save_best=True\n                    )],\n                    verbose=20\n                )\n\n                best_iter = model.best_iteration\n                best_score = model.best_score\n\n            print(f\"Best iteration: {best_iter}\")\n            print(f\"Best score: {best_score}\")\n\n            # ===== MLflow Metrics =====\n            mlflow.log_metric(\"best_iteration\", best_iter)\n            mlflow.log_metric(\"best_score\", best_score)\n\n            # ===== Save model locally =====\n            joblib.dump(model, f\"models/{model_name}_Fold_{i+1}.model\")\n\n            # ===== Log model =====\n            mlflow.sklearn.log_model(model,artifact_path=f\"{model_name}_Fold_{i+1}\")\n\n            models.append((model_name, i + 1, model))\n\n            del model\n            gc.collect()\n\n    del train_df, X_train, y_train, w_train\n    gc.collect()\n\n    if folds > 1:\n        START_TRAIN += 200\n        END_TRAIN   += 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T17:50:19.194096Z","iopub.execute_input":"2025-12-24T17:50:19.194838Z","iopub.status.idle":"2025-12-24T18:01:55.781783Z","shell.execute_reply.started":"2025-12-24T17:50:19.194809Z","shell.execute_reply":"2025-12-24T18:01:55.780960Z"}},"outputs":[{"name":"stderr","text":"2025/12/24 17:50:19 INFO mlflow.tracking.fluent: Experiment with name 'JS_Kaggle_Experiments2' does not exist. Creating a new experiment.\n","output_type":"stream"},{"name":"stdout","text":"Load train data and apply reduce memory function on Fold 1\ndf memory usage before reduce : 2522.8769760131836 MB\nMemory usage after optimization is: 1297.07 MB\nDecreased by 48.6%\n\n================ Fold 1/2 ================\nTrain dates: from day 1099 to 1298 (200 days)\n\n============== LightGBM | Fold 1 ==============\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9912\n[LightGBM] [Info] Number of data points in the train set: 7472960, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (570.14 MB) transferred to GPU in 0.599318 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.006229\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"2025/12/24 17:53:28 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n","output_type":"stream"},{"name":"stdout","text":"Early stopping, best iteration is:\n[74]\tvalid_0's l2: 0.595266\tvalid_0's weighted_zero_mean_r2: 0.00471052\nBest iteration: 74\nBest score: 0.0047105231385636825\n\n============== XGBoost | Fold 1 ==============\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00017\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00272\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00409\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00474\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00504\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00388\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00263\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00120\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00060\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:-0.00070\n","output_type":"stream"},{"name":"stderr","text":"2025/12/24 17:55:57 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n","output_type":"stream"},{"name":"stdout","text":"Best iteration: 81\nBest score: 0.005052\nLoad train data and apply reduce memory function on Fold 2\ndf memory usage before reduce : 2454.9030990600586 MB\nMemory usage after optimization is: 1262.13 MB\nDecreased by 48.6%\n\n================ Fold 2/2 ================\nTrain dates: from day 1299 to 1498 (200 days)\n\n============== LightGBM | Fold 2 ==============\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9913\n[LightGBM] [Info] Number of data points in the train set: 7271616, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (554.78 MB) transferred to GPU in 0.532090 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.003815\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"2025/12/24 17:59:23 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n","output_type":"stream"},{"name":"stdout","text":"Early stopping, best iteration is:\n[119]\tvalid_0's l2: 0.594277\tvalid_0's weighted_zero_mean_r2: 0.0062599\nBest iteration: 119\nBest score: 0.0062599037080367514\n\n============== XGBoost | Fold 2 ==============\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00017\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00270\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00425\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00524\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00578\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00605\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00622\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00619\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00609\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00591\n[200]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00572\n[220]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00549\n[232]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00536\n","output_type":"stream"},{"name":"stderr","text":"2025/12/24 18:01:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n","output_type":"stream"},{"name":"stdout","text":"Best iteration: 132\nBest score: 0.006234\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%capture\n!zip -r mlruns.zip /kaggle/working/mlruns\n\nfrom IPython.display import display, FileLink\nprint(\"ðŸ“¥ Download Links:\\n\")\ndisplay(FileLink('mlruns.zip', result_html_prefix=\" Track the models: \"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T18:03:45.862962Z","iopub.execute_input":"2025-12-24T18:03:45.863806Z","iopub.status.idle":"2025-12-24T18:03:46.374271Z","shell.execute_reply.started":"2025-12-24T18:03:45.863771Z","shell.execute_reply":"2025-12-24T18:03:46.373262Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"''' \n#### How to run mlflow ui on local env\n1- unzip mlruns.zip\n2- write in your terminal after install mlfow\n- cd Downloads/kaggle/working\n- mlflow ui --backend-store-uri file:./mlruns\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}