{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":11305158,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport polars as pl\nimport gc\nimport os\nimport joblib \nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost.callback import EarlyStopping\nfrom catboost import CatBoostRegressor\nimport kaggle_evaluation.jane_street_inference_server\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T05:58:47.801853Z","iopub.execute_input":"2025-12-15T05:58:47.802555Z","iopub.status.idle":"2025-12-15T05:58:54.029419Z","shell.execute_reply.started":"2025-12-15T05:58:47.802520Z","shell.execute_reply":"2025-12-15T05:58:54.028781Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ============================\n# Reduce Memory Usage Function\n# ============================\ndef reduce_memory_usage(df,float16_as32=False):\n    start_mem = df.memory_usage().sum()/1024**2\n    print(f'df memory usage before reduce : {start_mem} MB')\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # Skip non-numeric columns\n        if col_type.kind not in ['i','f']:\n            continue\n        \n        c_min = df[col].min()\n        c_max = df[col].max()\n\n        # Integer types\n        if col_type.kind in ['i']:\n            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            else:\n                df[col] = df[col].astype(np.int64)\n\n        # Float types\n        else:\n            if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float32 if float16_as32 else np.float16)\n            elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n        \n    end_mem = df.memory_usage().sum()/1024**2\n    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n    print(f\"Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T05:58:55.084714Z","iopub.execute_input":"2025-12-15T05:58:55.085243Z","iopub.status.idle":"2025-12-15T05:58:55.094877Z","shell.execute_reply.started":"2025-12-15T05:58:55.085222Z","shell.execute_reply":"2025-12-15T05:58:55.094113Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# -------------------------------------------\n# Custom Weighted Zero-Mean R² for Lgb Model\n# ------------------------------------------\ndef weighted_zero_mean_r2_lgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n\n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm) ** 2)\n    denominator = np.sum(sample_weight * (y_true_zm) ** 2)\n\n    r2 = 1 - numerator / (denominator + 1e-38)\n    return \"weighted_zero_mean_r2\", r2, True   # maximize=True\n# -------------------------------------------\n# Custom Weighted Zero-Mean R² for Xgb Model\n# -------------------------------------------\ndef weighted_zero_mean_r2_xgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n    \n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm)**2)\n    denominator = np.sum(sample_weight * (y_true_zm)**2)\n    \n    r2 = 1 - numerator / (denominator + 1e-38)\n    return r2        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T05:58:55.289618Z","iopub.execute_input":"2025-12-15T05:58:55.289920Z","iopub.status.idle":"2025-12-15T05:58:55.295644Z","shell.execute_reply.started":"2025-12-15T05:58:55.289901Z","shell.execute_reply":"2025-12-15T05:58:55.294857Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================\n#  Model Dictionary\n# ============================\nmodel_dict = {\n    \"LightGBM\": lambda:LGBMRegressor(\n    n_estimators=3000,\n    learning_rate=0.01,\n    num_leaves=50,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    max_bin=128,\n    device=\"gpu\"\n    ),\n\n    \"XGBoost\": lambda: XGBRegressor(\n    n_estimators=2000,\n    learning_rate=0.01,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"reg:squarederror\",\n    device=\"cuda\",\n    tree_method=\"gpu_hist\",\n    max_bin=128,\n    random_state=42,\n    eval_metric=weighted_zero_mean_r2_xgb,\n    disable_default_eval_metric=True\n    ),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T05:58:55.453715Z","iopub.execute_input":"2025-12-15T05:58:55.453990Z","iopub.status.idle":"2025-12-15T05:58:55.458659Z","shell.execute_reply.started":"2025-12-15T05:58:55.453970Z","shell.execute_reply":"2025-12-15T05:58:55.457915Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# use the Kaggle input directory\ntrain_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet'\n\n# Responders_Columns\nfeatures_cols = [f\"feature_{i:02d}\"for i in range(79)]\n\n# Define the target \ntarget = 'responder_6'\n\n#  Create models directory \nos.makedirs(\"models\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T05:58:55.809680Z","iopub.execute_input":"2025-12-15T05:58:55.810456Z","iopub.status.idle":"2025-12-15T05:58:55.814208Z","shell.execute_reply.started":"2025-12-15T05:58:55.810428Z","shell.execute_reply":"2025-12-15T05:58:55.813395Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Preapare Valid_df","metadata":{}},{"cell_type":"code","source":"# prepare valid_df\nskip_dates= 1499  # I will use last 200 days for validation\nvalid_df = pd.read_parquet(train_path, filters=[('date_id','>=', skip_dates)])\nvalid_df = reduce_memory_usage(valid_df)\n\n# X,y,w \nX_valid = valid_df[features_cols + ['time_id']]\ny_valid = (valid_df[target]+ 0.5 * valid_df['responder_7']+ .5 * valid_df['responder_8'])\nw_valid =  valid_df[\"weight\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T05:58:56.168834Z","iopub.execute_input":"2025-12-15T05:58:56.169342Z","iopub.status.idle":"2025-12-15T05:59:08.931752Z","shell.execute_reply.started":"2025-12-15T05:58:56.169320Z","shell.execute_reply":"2025-12-15T05:59:08.931122Z"}},"outputs":[{"name":"stdout","text":"df memory usage before reduce : 2510.1318740844727 MB\nMemory usage after optimization is: 1290.52 MB\nDecreased by 48.6%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Prepare train data & Train models","metadata":{}},{"cell_type":"code","source":"START_TRAIN = 1200\nEND_TRAIN   = 1399\nfolds = 2\n\nmodels = []\n\nfor i in range(folds):\n    print(f'Load train data and apply reduce memory function on Fold {i+1}')\n   # load train data\n    train_df = pd.read_parquet(\n        train_path,\n        filters=[[('date_id', '>=', START_TRAIN),\n                  ('date_id', '<=', END_TRAIN)]]\n    )\n    train_df = reduce_memory_usage(train_df)\n\n    # X,y,w\n    X_train = train_df[features_cols + ['time_id']]\n    y_train = (\n        train_df[target]\n        + 0.5 * train_df['responder_7']\n        + 0.5 * train_df['responder_8']\n    )\n    w_train = train_df[\"weight\"]\n    \n    print(f\"\\n================ Fold {i+1}/{folds} ================\")\n    print(f\"Train dates: from day {train_df['date_id'].min()} to {train_df['date_id'].max()} ({train_df['date_id'].nunique()} days)\")\n\n    # Train and evulate models\n    for model_name, model_class in model_dict.items():\n\n        print(f'\\n============== {model_name} | Fold {i+1} =========')\n\n        model = model_class()\n\n        if model_name == \"LightGBM\":\n\n            model.fit(\n                X_train, y_train,\n                sample_weight=w_train,\n                eval_set=[(X_valid, y_valid)],\n                eval_sample_weight=[w_valid],\n                eval_metric=weighted_zero_mean_r2_lgb,\n                callbacks=[lgb.early_stopping(100)]\n            )\n\n            print(\"Best iteration:\", model.best_iteration_)\n            print(\n                \"Best score:\",\n                model.best_score_['valid_0']['weighted_zero_mean_r2']\n            )\n\n        else:  # XGBoost\n\n            model.fit(\n                X_train, y_train,\n                sample_weight=w_train,\n                eval_set=[(X_valid, y_valid)],\n                sample_weight_eval_set=[w_valid],\n                callbacks=[EarlyStopping(rounds=100, maximize=True, save_best=True)],\n                verbose=20\n            )\n\n            print(f\"Best iteration: {model.best_iteration}\")\n            print(f\"Best CV score: {model.best_score}\\n\")\n\n        joblib.dump(model, f\"models/{model_name}_{i+1}.model\")\n        models.append((model_name, i+1, model))\n\n        del model\n        gc.collect()\n\n    del train_df, X_train, y_train, w_train\n    gc.collect()\n\n    START_TRAIN += 200\n    END_TRAIN   += 200\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T05:59:08.932724Z","iopub.execute_input":"2025-12-15T05:59:08.933069Z","iopub.status.idle":"2025-12-15T06:36:32.009322Z","shell.execute_reply.started":"2025-12-15T05:59:08.933045Z","shell.execute_reply":"2025-12-15T06:36:32.008683Z"}},"outputs":[{"name":"stdout","text":"df memory usage before reduce : 2494.772392272949 MB\nMemory usage after optimization is: 1282.62 MB\nDecreased by 48.6%\n\n================ Fold 1/2 ================\nTrain dates: from day 1200 to 1399 (7389712 days)\n============== LightGBM | Fold 1 =========\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9913\n[LightGBM] [Info] Number of data points in the train set: 7389712, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (563.79 MB) transferred to GPU in 0.587026 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.006386\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[376]\tvalid_0's l2: 1.49738\tvalid_0's weighted_zero_mean_r2: 0.0107017\nBest iteration: 376\nBest score: 0.010701737129045652\n============== XGBoost | Fold 1 =========\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00012\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00234\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00408\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00547\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00655\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00741\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00805\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00858\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00907\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00946\n[200]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00978\n[220]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01008\n[240]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01018\n[260]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01039\n[280]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01049\n[300]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01057\n[320]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01058\n[340]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01060\n[360]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01069\n[380]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01057\n[400]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01043\n[420]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01044\n[440]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01047\n[460]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01045\n[468]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01047\nBest iteration: 368\nBest CV score: 0.010734\ndf memory usage before reduce : 2468.955390930176 MB\nMemory usage after optimization is: 1269.35 MB\nDecreased by 48.6%\n\n================ Fold 2/2 ================\nTrain dates: from day 1400 to 1599 (7313240 days)\n============== LightGBM | Fold 2 =========\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9914\n[LightGBM] [Info] Number of data points in the train set: 7313240, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (557.96 MB) transferred to GPU in 0.517964 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score -0.002058\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2000]\tvalid_0's l2: 1.38491\tvalid_0's weighted_zero_mean_r2: 0.084972\nBest iteration: 2000\nBest score: 0.08497201964694978\n============== XGBoost | Fold 2 =========\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00014\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00279\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00491\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00663\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00821\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00970\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01120\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01245\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01418\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01542\n[200]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01657\n[220]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01766\n[240]\tvalidation_0-weighted_zero_mean_r2_xgb:0.01889\n[260]\tvalidation_0-weighted_zero_mean_r2_xgb:0.02007\n[280]\tvalidation_0-weighted_zero_mean_r2_xgb:0.02105\n[300]\tvalidation_0-weighted_zero_mean_r2_xgb:0.02190\n[320]\tvalidation_0-weighted_zero_mean_r2_xgb:0.02274\n[340]\tvalidation_0-weighted_zero_mean_r2_xgb:0.02383\n[360]\tvalidation_0-weighted_zero_mean_r2_xgb:0.02479\n[380]\tvalidation_0-weighted_zero_mean_r2_xgb:0.02627\n[400]\tvalidation_0-weighted_zero_mean_r2_xgb:0.02735\n[420]\tvalidation_0-weighted_zero_mean_r2_xgb:0.02846\n[440]\tvalidation_0-weighted_zero_mean_r2_xgb:0.02979\n[460]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03076\n[480]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03179\n[500]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03275\n[520]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03354\n[540]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03428\n[560]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03530\n[580]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03604\n[600]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03688\n[620]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03756\n[640]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03829\n[660]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03905\n[680]\tvalidation_0-weighted_zero_mean_r2_xgb:0.03992\n[700]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04032\n[720]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04097\n[740]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04172\n[760]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04242\n[780]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04301\n[800]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04371\n[820]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04432\n[840]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04508\n[860]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04544\n[880]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04598\n[900]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04645\n[920]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04698\n[940]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04756\n[960]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04809\n[980]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04878\n[1000]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04926\n[1020]\tvalidation_0-weighted_zero_mean_r2_xgb:0.04963\n[1040]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05000\n[1060]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05062\n[1080]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05104\n[1100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05147\n[1120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05191\n[1140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05230\n[1160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05283\n[1180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05344\n[1200]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05402\n[1220]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05451\n[1240]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05513\n[1260]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05570\n[1280]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05596\n[1300]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05617\n[1320]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05639\n[1340]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05695\n[1360]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05737\n[1380]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05777\n[1400]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05815\n[1420]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05854\n[1440]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05886\n[1460]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05908\n[1480]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05934\n[1500]\tvalidation_0-weighted_zero_mean_r2_xgb:0.05975\n[1520]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06009\n[1540]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06037\n[1560]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06078\n[1580]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06112\n[1600]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06144\n[1620]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06176\n[1640]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06207\n[1660]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06250\n[1680]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06291\n[1700]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06316\n[1720]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06350\n[1740]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06378\n[1760]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06411\n[1780]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06453\n[1800]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06481\n[1820]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06507\n[1840]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06541\n[1860]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06568\n[1880]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06596\n[1900]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06635\n[1920]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06656\n[1940]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06682\n[1960]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06711\n[1980]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06729\n[1999]\tvalidation_0-weighted_zero_mean_r2_xgb:0.06739\nBest iteration: 1999\nBest CV score: 0.067393\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}