{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":11305158,"sourceType":"competition"},{"sourceId":669416,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":506904,"modelId":521644}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport polars as pl\nimport gc\nimport os\nimport joblib \nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost.callback import EarlyStopping\nfrom catboost import CatBoostRegressor\nimport kaggle_evaluation.jane_street_inference_server\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:36:44.998059Z","iopub.execute_input":"2025-12-10T01:36:44.998573Z","iopub.status.idle":"2025-12-10T01:36:51.251688Z","shell.execute_reply.started":"2025-12-10T01:36:44.998548Z","shell.execute_reply":"2025-12-10T01:36:51.251067Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# use the Kaggle input directory\ntrain_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet'\n\n# Define the feature names based on the number of features (79 in this case)\nfeatures_names = [f\"feature_{i:02d}\"for i in range(79)]\n\n# Define the target \ntarget = 'responder_6'\n\n# Skip_dates\nskip_dates = 1200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:36:54.798297Z","iopub.execute_input":"2025-12-10T01:36:54.798877Z","iopub.status.idle":"2025-12-10T01:36:54.803047Z","shell.execute_reply.started":"2025-12-10T01:36:54.798850Z","shell.execute_reply":"2025-12-10T01:36:54.802310Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ============================\n# Reduce Memory Usage Function\n# ============================\ndef reduce_memory_usage(df,float16_as32=False):\n    start_mem = df.memory_usage().sum()/1024**2\n    print(f'df memory usage before reduce : {start_mem} MB')\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # Skip non-numeric columns\n        if col_type.kind not in ['i','f']:\n            continue\n        \n        c_min = df[col].min()\n        c_max = df[col].max()\n\n        # Integer types\n        if col_type.kind in ['i']:\n            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            else:\n                df[col] = df[col].astype(np.int64)\n\n        # Float types\n        else:\n            if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float32 if float16_as32 else np.float16)\n            elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n        \n    end_mem = df.memory_usage().sum()/1024**2\n    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n    print(f\"Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:36:57.746374Z","iopub.execute_input":"2025-12-10T01:36:57.747048Z","iopub.status.idle":"2025-12-10T01:36:57.882756Z","shell.execute_reply.started":"2025-12-10T01:36:57.747022Z","shell.execute_reply":"2025-12-10T01:36:57.881982Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df = pd.read_parquet(train_path, filters=[('date_id','>=', skip_dates)])\ndf = reduce_memory_usage(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:37:00.347094Z","iopub.execute_input":"2025-12-10T01:37:00.347387Z","iopub.status.idle":"2025-12-10T01:37:24.845707Z","shell.execute_reply.started":"2025-12-10T01:37:00.347363Z","shell.execute_reply":"2025-12-10T01:37:24.844925Z"}},"outputs":[{"name":"stdout","text":"df memory usage before reduce : 6205.884635925293 MB\nMemory usage after optimization is: 3190.60 MB\nDecreased by 48.6%\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"START_TRAIN = 1200\nEND_TRAIN   = 1599\nVALID_START = 1600\nVALID_END   = 1698\nN_FOLDS     = 2\n\n# ===============================#\n#     LOAD DATA (Train + Final Valid)\n# ===============================#\ntrain_df = ( df[df[\"date_id\"].between(START_TRAIN, END_TRAIN)].sort_values(\"date_id\"))\n\nvalid_df = df[df[\"date_id\"].between(VALID_START, VALID_END)]\nX_valid = valid_df[features_names]\ny_valid = valid_df[target]\nw_valid =  valid_df[\"weight\"]\n# ===============================#\n#     CREATE FOLDS FROM DATES\n# ===============================#\nall_dates = np.arange(START_TRAIN, END_TRAIN+1)\nfolds = np.array_split(all_dates, N_FOLDS)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:37:33.271043Z","iopub.execute_input":"2025-12-10T01:37:33.271593Z","iopub.status.idle":"2025-12-10T01:37:45.440425Z","shell.execute_reply.started":"2025-12-10T01:37:33.271567Z","shell.execute_reply":"2025-12-10T01:37:45.439837Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"del df \ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:38:33.114103Z","iopub.execute_input":"2025-12-10T01:38:33.114837Z","iopub.status.idle":"2025-12-10T01:38:33.255932Z","shell.execute_reply.started":"2025-12-10T01:38:33.114809Z","shell.execute_reply":"2025-12-10T01:38:33.255287Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# -------------------------------------------\n# Custom Weighted Zero-Mean R² for Lgb Model\n# ------------------------------------------\ndef weighted_zero_mean_r2_lgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n\n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm) ** 2)\n    denominator = np.sum(sample_weight * (y_true_zm) ** 2)\n\n    r2 = 1 - numerator / (denominator + 1e-38)\n    return \"weighted_zero_mean_r2\", r2, True   # maximize=True\n# -------------------------------------------\n# Custom Weighted Zero-Mean R² for Xgb Model\n# -------------------------------------------\ndef weighted_zero_mean_r2_xgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n    \n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm)**2)\n    denominator = np.sum(sample_weight * (y_true_zm)**2)\n    \n    r2 = 1 - numerator / (denominator + 1e-38)\n    return r2        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:39:21.109818Z","iopub.execute_input":"2025-12-10T01:39:21.110105Z","iopub.status.idle":"2025-12-10T01:39:21.116196Z","shell.execute_reply.started":"2025-12-10T01:39:21.110073Z","shell.execute_reply":"2025-12-10T01:39:21.115375Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ============================\n#  Model Dictionary\n# ============================\nmodel_dict = {\n    \"LightGBM\": lambda:LGBMRegressor(\n    n_estimators=2000,\n    learning_rate=0.01,\n    num_leaves=50,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    max_bin=128,\n    device=\"gpu\"\n    ),\n\n    \"XGBoost\": lambda: XGBRegressor(\n    n_estimators=2000,\n    learning_rate=0.01,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"reg:squarederror\",\n    device=\"cuda\",\n    tree_method=\"gpu_hist\",\n    max_bin=128,\n    random_state=42,\n    eval_metric=weighted_zero_mean_r2_xgb,\n    disable_default_eval_metric=True\n    ),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:46:31.598813Z","iopub.execute_input":"2025-12-10T01:46:31.599374Z","iopub.status.idle":"2025-12-10T01:46:31.603958Z","shell.execute_reply.started":"2025-12-10T01:46:31.599349Z","shell.execute_reply":"2025-12-10T01:46:31.603336Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ==========================\n#  Create models directory \n# ==========================\nos.makedirs(\"models\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:39:27.521215Z","iopub.execute_input":"2025-12-10T01:39:27.521545Z","iopub.status.idle":"2025-12-10T01:39:27.525854Z","shell.execute_reply.started":"2025-12-10T01:39:27.521519Z","shell.execute_reply":"2025-12-10T01:39:27.525073Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ===============================#\n#     LOOP THROUGH FOLDS\n# ===============================#\nmodels = []\n\nfor fold, train_dates in enumerate(folds, start=1):\n    print(f\"--------------------- Fold {fold}/{N_FOLDS} -------------------\")\n    print(f\"Train dates: from day {train_dates.min()} to {train_dates.max()} ({len(train_dates)} days)\")\n    print('-'*50)\n    \n    fold_df = train_df[train_df[\"date_id\"].isin(train_dates)]\n    X_train = fold_df[features_names]\n    y_train = fold_df[target]\n    w_train = fold_df[\"weight\"]\n\n    \n    for model_name, model_class in model_dict.items():\n        \n        if model_name ==\"LightGBM\":\n            print(f'============== {model_name} with Fold {fold}/{N_FOLDS} =========')\n            # create NEW model object for THIS fold\n            model = model_class()\n            model.fit(X_train,y_train,\n            sample_weight=w_train,\n            eval_set=[(X_valid, y_valid)],\n            eval_sample_weight=[w_valid],\n            eval_metric=weighted_zero_mean_r2_lgb,\n            callbacks=[lgb.early_stopping(stopping_rounds=100)])\n        \n        else:\n            print(f'============== {model_name} with Fold {fold}/{N_FOLDS} =========')\n            # create NEW model object for THIS fold\n            model = model_class()\n            model.fit(\n            X_train,\n            y_train,\n            sample_weight=w_train,\n            eval_set=[(X_valid, y_valid)],\n            sample_weight_eval_set=[w_valid],\n            verbose=20,\n            callbacks=[EarlyStopping(rounds=100, maximize=True, save_best=True)],)\n\n        \n        # Save model\n        joblib.dump(model, f\"models/{model_name}_{fold}.model\")\n        models.append((model_name, fold, model))\n        \n        # Delete model to free memory\n        del model\n        gc.collect()\n\n    del X_train, y_train, w_train\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T01:46:36.571259Z","iopub.execute_input":"2025-12-10T01:46:36.571957Z","iopub.status.idle":"2025-12-10T02:04:03.755839Z","shell.execute_reply.started":"2025-12-10T01:46:36.571932Z","shell.execute_reply":"2025-12-10T02:04:03.755219Z"}},"outputs":[{"name":"stdout","text":"--------------------- Fold 1/2 -------------------\nTrain dates: from day 1200 to 1399 (200 days)\n--------------------------------------------------\n============== LightGBM with Fold 1/2 =========\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9785\n[LightGBM] [Info] Number of data points in the train set: 7389712, number of used features: 79\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 79 dense feature groups (563.79 MB) transferred to GPU in 0.568625 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.000885\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[566]\tvalid_0's l2: 0.622334\tvalid_0's weighted_zero_mean_r2: 0.00527403\n============== XGBoost with Fold 1/2 =========\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00008\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00131\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00219\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00287\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00338\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00377\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00407\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00431\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00450\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00464\n[200]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00477\n[220]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00487\n[240]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00495\n[260]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00503\n[280]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00507\n[300]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00509\n[320]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00511\n[340]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00511\n[360]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00513\n[380]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00515\n[400]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00517\n[420]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00517\n[440]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00517\n[460]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00510\n[480]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00503\n[500]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00498\n[520]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00496\n[532]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00490\n--------------------- Fold 2/2 -------------------\nTrain dates: from day 1400 to 1599 (200 days)\n--------------------------------------------------\n============== LightGBM with Fold 2/2 =========\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9786\n[LightGBM] [Info] Number of data points in the train set: 7313240, number of used features: 79\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 79 dense feature groups (557.96 MB) transferred to GPU in 0.547397 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score -0.001981\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[774]\tvalid_0's l2: 0.621888\tvalid_0's weighted_zero_mean_r2: 0.00591767\n============== XGBoost with Fold 2/2 =========\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00007\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00125\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00208\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00275\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00325\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00367\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00401\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00427\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00448\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00465\n[200]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00480\n[220]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00493\n[240]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00505\n[260]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00517\n[280]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00526\n[300]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00533\n[320]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00543\n[340]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00548\n[360]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00553\n[380]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00559\n[400]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00562\n[420]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00567\n[440]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00570\n[460]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00576\n[480]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00576\n[500]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00575\n[520]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00572\n[540]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00572\n[560]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00570\n[570]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00570\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ======================\n# Load Pretrained model \n# ======================\n'''\nmodels_path ='/kaggle/input/models/scikitlearn/default/1/models'\nfor file in os.listdir(models_path):\n    if file.endswith(\".model\"):\n        model = joblib.load(os.path.join(models_path, file))\n        models.append(model)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# Prediction Using the Ensemble of Models\n# ========================================\n\nlags_: pl.DataFrame | None = None\n\ndef predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame:\n    \"\"\"Make a prediction using the ensemble of models.\"\"\"\n    global lags_\n    if lags is not None:\n        lags_ = lags\n\n    # Convert features to NumPy for model prediction\n    feat = test.select(features_names).to_numpy()\n    \n    # Ensemble prediction (average over all models)\n    pred = np.mean([model.predict(feat) for model in models], axis=0)\n    \n    # Create Polars DataFrame for submission\n    predictions = pl.DataFrame({\n        'row_id': test['row_id'],\n        'responder_6': pred.astype(np.float32)\n    })\n    \n    # Assertions for safety\n    assert isinstance(predictions, pl.DataFrame)\n    assert list(predictions.columns) == ['row_id', 'responder_6']\n    assert len(predictions) == len(test)\n\n    return predictions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============\n# Submission\n# ===============\ninference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    # If the competition is currently running on Kaggle\n    inference_server.serve()\n\nelif os.getenv('KAGGLE_IS_COMPETITION_ACTIVE'):\n    # If the competition is still active\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/jane-street-realtime-market-data-forecasting/test.parquet',\n            '/kaggle/input/jane-street-realtime-market-data-forecasting/lags.parquet',\n        )\n    )\n\nelse:\n    # After the competition has ended\n    test_df = pl.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet')\n    lags_df = pl.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet/date_id=0/part-0.parquet')\n    \n    predictions = predict(test_df, lags_df)\n    print(predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}