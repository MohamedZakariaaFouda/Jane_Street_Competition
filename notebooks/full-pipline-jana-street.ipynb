{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84493,"databundleVersionId":11305158,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport gc\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:29:03.007384Z","iopub.execute_input":"2025-11-26T14:29:03.007677Z","iopub.status.idle":"2025-11-26T14:29:03.011978Z","shell.execute_reply.started":"2025-11-26T14:29:03.007655Z","shell.execute_reply":"2025-11-26T14:29:03.011228Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# use the Kaggle input directory\ntrain_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet'\n\n# Define the feature names based on the number of features (79 in this case)\nfeatures_names = [f\"feature_{i:02d}\"for i in range(79)]\n\n# Define the target \ntarget = 'responder_6'\n\n# Skip_dates\nskip_dates = 1400","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T13:56:35.906513Z","iopub.execute_input":"2025-11-26T13:56:35.907165Z","iopub.status.idle":"2025-11-26T13:56:35.911146Z","shell.execute_reply.started":"2025-11-26T13:56:35.907137Z","shell.execute_reply":"2025-11-26T13:56:35.910421Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ============================\n# Reduce Memory Usage Function\n# ============================\ndef reduce_memory_usage(df,float16_as32=False):\n    start_mem = df.memory_usage().sum()/1024**2\n    print(f'df memory usage before reduce : {start_mem} MB')\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # Skip non-numeric columns\n        if col_type.kind not in ['i','f']:\n            continue\n        \n        c_min = df[col].min()\n        c_max = df[col].max()\n\n        # Integer types\n        if col_type.kind in ['i']:\n            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            else:\n                df[col] = df[col].astype(np.int64)\n\n        # Float types\n        else:\n            if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float32 if float16_as32 else np.float16)\n            elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n        \n    end_mem = df.memory_usage().sum()/1024**2\n    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n    print(f\"Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T13:56:36.695602Z","iopub.execute_input":"2025-11-26T13:56:36.696300Z","iopub.status.idle":"2025-11-26T13:56:36.703564Z","shell.execute_reply.started":"2025-11-26T13:56:36.696276Z","shell.execute_reply":"2025-11-26T13:56:36.703016Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df = pd.read_parquet(train_path, filters=[('date_id','>=', skip_dates)])\ndf = reduce_memory_usage(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T13:56:37.963105Z","iopub.execute_input":"2025-11-26T13:56:37.963733Z","iopub.status.idle":"2025-11-26T13:56:52.412030Z","shell.execute_reply.started":"2025-11-26T13:56:37.963710Z","shell.execute_reply":"2025-11-26T13:56:52.411334Z"}},"outputs":[{"name":"stdout","text":"df memory usage before reduce : 3711.112632751465 MB\nMemory usage after optimization is: 1907.97 MB\nDecreased by 48.6%\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:25:55.525266Z","iopub.execute_input":"2025-11-26T14:25:55.525551Z","iopub.status.idle":"2025-11-26T14:25:55.530382Z","shell.execute_reply.started":"2025-11-26T14:25:55.525527Z","shell.execute_reply":"2025-11-26T14:25:55.529670Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"START_TRAIN = 1500\nEND_TRAIN   = 1599\nVALID_START = 1650\nVALID_END   = 1698\nN_FOLDS     = 2\n\n# ===============================#\n#     LOAD DATA (Train + Final Valid)\n# ===============================#\ntrain_df = ( df[df[\"date_id\"].between(START_TRAIN, END_TRAIN)].sort_values(\"date_id\"))\n\nvalid_df = df[df[\"date_id\"].between(VALID_START, VALID_END)]\nX_valid = valid_df[features_names]\ny_valid = valid_df[\"responder_6\"]\nw_valid =  valid_df[\"weight\"]\n# ===============================#\n#     CREATE FOLDS FROM DATES\n# ===============================#\nall_dates = np.arange(START_TRAIN, END_TRAIN+1)\nfolds = np.array_split(all_dates, N_FOLDS)   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================\n#  Model Dictionary\n# ============================\nmodel_dict = {\n    \"LightGBM\": lambda: LGBMRegressor(n_estimators=50, learning_rate=0.1),\n    \"XGBoost\":  lambda: XGBRegressor(n_estimators=50, learning_rate=0.1,),\n    \"CatBoost\": lambda: CatBoostRegressor(iterations=50, learning_rate=0.1, verbose=False)\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================#\n#     LOOP THROUGH FOLDS\n# ===============================#\nmodels = []\n\nfor fold, train_dates in enumerate(folds, start=1):\n    print(f\"--------------------- Fold {fold}/{N_FOLDS} -------------------\")\n    print(f\"Train dates: from day {train_dates.min()} to {train_dates.max()} ({len(train_dates)} days)\")\n    print('-'*50)\n    \n    fold_df = train_df[train_df[\"date_id\"].isin(train_dates)]\n    X_train = fold_df[features_names]\n    y_train = fold_df[\"responder_6\"]\n    w_train = fold_df[\"weight\"]\n\n    \n    # IMPORTANT: create new model instance inside the fold\n    for model_name, model_class in model_dict.items():\n        \n        print(f'============== {model_name} with Fold {fold}/{N_FOLDS} =========')\n        # create NEW model object for THIS fold\n        model = model_class()\n        model.fit(X_train, y_train, sample_weight=w_train)\n        models.append((model_name, fold, model))\n\n        \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:45:51.468212Z","iopub.execute_input":"2025-11-26T14:45:51.469005Z","iopub.status.idle":"2025-11-26T14:49:13.668230Z","shell.execute_reply.started":"2025-11-26T14:45:51.468975Z","shell.execute_reply":"2025-11-26T14:49:13.667377Z"}},"outputs":[{"name":"stdout","text":"--------------------- Fold 1/2 -------------------\nTrain dates: from day 1500 to 1549 (50 days)\n--------------------------------------------------\n============== LightGBM with Fold 1/2 =========\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.742961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 19236\n[LightGBM] [Info] Number of data points in the train set: 1862432, number of used features: 79\n[LightGBM] [Info] Start training from score -0.011091\n============== XGBoost with Fold 1/2 =========\n============== CatBoost with Fold 1/2 =========\n--------------------- Fold 2/2 -------------------\nTrain dates: from day 1550 to 1599 (50 days)\n--------------------------------------------------\n============== LightGBM with Fold 2/2 =========\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.744907 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 19234\n[LightGBM] [Info] Number of data points in the train set: 1855656, number of used features: 79\n[LightGBM] [Info] Start training from score 0.000357\n============== XGBoost with Fold 2/2 =========\n============== CatBoost with Fold 2/2 =========\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}