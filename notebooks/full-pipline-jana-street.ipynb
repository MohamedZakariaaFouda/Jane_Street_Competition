{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":11305158,"sourceType":"competition"},{"sourceId":669416,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":506904,"modelId":521644}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport polars as pl\nimport gc\nimport os\nimport joblib \nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost.callback import EarlyStopping\nfrom catboost import CatBoostRegressor\nimport kaggle_evaluation.jane_street_inference_server\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:50:58.646801Z","iopub.execute_input":"2025-12-14T05:50:58.647085Z","iopub.status.idle":"2025-12-14T05:51:04.726032Z","shell.execute_reply.started":"2025-12-14T05:50:58.647063Z","shell.execute_reply":"2025-12-14T05:51:04.725483Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"'''\n#add axurailty target \n#use weight and basis to track the model \n#after finish ensure to fit sumbission features \n#edit data sciencs life cycle by adding features engneering \n- add auxailrty target\n- drop features has no correlatio after make shape values\n- drop the outlier day \n- treat with missing value\n- create featuter like rolling mean and std mean and so on\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:51:04.727038Z","iopub.execute_input":"2025-12-14T05:51:04.727544Z","iopub.status.idle":"2025-12-14T05:51:04.733148Z","shell.execute_reply.started":"2025-12-14T05:51:04.727524Z","shell.execute_reply":"2025-12-14T05:51:04.732472Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\n#add axurailty target \\n#use weight and basis to track the model \\n'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# use the Kaggle input directory\ntrain_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet'\n\n# Responders_Columns\nfeatures_cols = [f\"feature_{i:02d}\"for i in range(79)]\n\n# Define the target \ntarget = 'responder_6'\n\n# Skip_dates\nskip_dates = 1200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:51:04.733785Z","iopub.execute_input":"2025-12-14T05:51:04.733985Z","iopub.status.idle":"2025-12-14T05:51:04.750857Z","shell.execute_reply.started":"2025-12-14T05:51:04.733970Z","shell.execute_reply":"2025-12-14T05:51:04.750161Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================\n# Reduce Memory Usage Function\n# ============================\ndef reduce_memory_usage(df,float16_as32=False):\n    start_mem = df.memory_usage().sum()/1024**2\n    print(f'df memory usage before reduce : {start_mem} MB')\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # Skip non-numeric columns\n        if col_type.kind not in ['i','f']:\n            continue\n        \n        c_min = df[col].min()\n        c_max = df[col].max()\n\n        # Integer types\n        if col_type.kind in ['i']:\n            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            else:\n                df[col] = df[col].astype(np.int64)\n\n        # Float types\n        else:\n            if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float32 if float16_as32 else np.float16)\n            elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n        \n    end_mem = df.memory_usage().sum()/1024**2\n    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n    print(f\"Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:51:04.752446Z","iopub.execute_input":"2025-12-14T05:51:04.752676Z","iopub.status.idle":"2025-12-14T05:51:04.867969Z","shell.execute_reply.started":"2025-12-14T05:51:04.752660Z","shell.execute_reply":"2025-12-14T05:51:04.867398Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df = pd.read_parquet(train_path, filters=[('date_id','>=', skip_dates)])\ndf = reduce_memory_usage(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:51:04.868725Z","iopub.execute_input":"2025-12-14T05:51:04.868950Z","iopub.status.idle":"2025-12-14T05:51:28.553822Z","shell.execute_reply.started":"2025-12-14T05:51:04.868927Z","shell.execute_reply":"2025-12-14T05:51:28.553039Z"}},"outputs":[{"name":"stdout","text":"df memory usage before reduce : 6205.884635925293 MB\nMemory usage after optimization is: 3190.60 MB\nDecreased by 48.6%\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"START_TRAIN = 1200\nEND_TRAIN   = 1599\nVALID_START = 1600\nVALID_END   = 1698\nN_FOLDS     = 2\n\n# ===============================#\n#     LOAD DATA (Train + Final Valid)\n# ===============================#\ntrain_df = ( df[df[\"date_id\"].between(START_TRAIN, END_TRAIN)].sort_values(\"date_id\"))\n\nvalid_df = df[df[\"date_id\"].between(VALID_START, VALID_END)]\nX_valid = valid_df[features_cols + ['time_id']]\ny_valid = valid_df[target]\nw_valid =  valid_df[\"weight\"]\n# ===============================#\n#     CREATE FOLDS FROM DATES\n# ===============================#\nall_dates = np.arange(START_TRAIN, END_TRAIN+1)\nfolds = np.array_split(all_dates, N_FOLDS)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:51:28.555393Z","iopub.execute_input":"2025-12-14T05:51:28.555606Z","iopub.status.idle":"2025-12-14T05:51:40.475504Z","shell.execute_reply.started":"2025-12-14T05:51:28.555589Z","shell.execute_reply":"2025-12-14T05:51:40.474903Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# -------------------------------------------\n# Custom Weighted Zero-Mean R² for Lgb Model\n# ------------------------------------------\ndef weighted_zero_mean_r2_lgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n\n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm) ** 2)\n    denominator = np.sum(sample_weight * (y_true_zm) ** 2)\n\n    r2 = 1 - numerator / (denominator + 1e-38)\n    return \"weighted_zero_mean_r2\", r2, True   # maximize=True\n# -------------------------------------------\n# Custom Weighted Zero-Mean R² for Xgb Model\n# -------------------------------------------\ndef weighted_zero_mean_r2_xgb(y_true, y_pred, sample_weight):\n    y_true_zm = y_true - np.average(y_true, weights=sample_weight)\n    y_pred_zm = y_pred - np.average(y_pred, weights=sample_weight)\n    \n    numerator = np.sum(sample_weight * (y_true_zm - y_pred_zm)**2)\n    denominator = np.sum(sample_weight * (y_true_zm)**2)\n    \n    r2 = 1 - numerator / (denominator + 1e-38)\n    return r2        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:51:40.612766Z","iopub.execute_input":"2025-12-14T05:51:40.613092Z","iopub.status.idle":"2025-12-14T05:51:40.629543Z","shell.execute_reply.started":"2025-12-14T05:51:40.613068Z","shell.execute_reply":"2025-12-14T05:51:40.628882Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ============================\n#  Model Dictionary\n# ============================\nmodel_dict = {\n    \"LightGBM\": lambda:LGBMRegressor(\n    n_estimators=2000,\n    learning_rate=0.01,\n    num_leaves=50,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    max_bin=128,\n    device=\"gpu\"\n    ),\n\n    \"XGBoost\": lambda: XGBRegressor(\n    n_estimators=2000,\n    learning_rate=0.01,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    objective=\"reg:squarederror\",\n    device=\"cuda\",\n    tree_method=\"gpu_hist\",\n    max_bin=128,\n    random_state=42,\n    eval_metric=weighted_zero_mean_r2_xgb,\n    disable_default_eval_metric=True\n    ),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:51:40.630177Z","iopub.execute_input":"2025-12-14T05:51:40.630376Z","iopub.status.idle":"2025-12-14T05:51:40.645620Z","shell.execute_reply.started":"2025-12-14T05:51:40.630362Z","shell.execute_reply":"2025-12-14T05:51:40.644788Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ==========================\n#  Create models directory \n# ==========================\nos.makedirs(\"models\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:51:40.647991Z","iopub.execute_input":"2025-12-14T05:51:40.648385Z","iopub.status.idle":"2025-12-14T05:51:40.665217Z","shell.execute_reply.started":"2025-12-14T05:51:40.648362Z","shell.execute_reply":"2025-12-14T05:51:40.664391Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ===============================#\n#     LOOP THROUGH FOLDS\n# ===============================#\nmodels = []\n\nfor fold, train_dates in enumerate(folds, start=1):\n    print(f\"--------------------- Fold {fold}/{N_FOLDS} -------------------\")\n    print(f\"Train dates: from day {train_dates.min()} to {train_dates.max()} ({len(train_dates)} days)\")\n    print('-'*50)\n    \n    fold_df = train_df[train_df[\"date_id\"].isin(train_dates)]\n    X_train = fold_df[features_cols + ['time_id']]\n    y_train = (fold_df[target]+ 0.5 * fold_df['responder_7']+ 0.5 * fold_df['responder_8'])\n    w_train = fold_df[\"weight\"]\n\n    \n    for model_name, model_class in model_dict.items():\n        \n        if model_name ==\"LightGBM\":\n            print(f'============== {model_name} with Fold {fold}/{N_FOLDS} =========')\n            # create NEW model object for THIS fold\n            model = model_class()\n            model.fit(X_train,y_train,\n            sample_weight=w_train,\n            eval_set=[(X_valid, y_valid)],\n            eval_sample_weight=[w_valid],\n            eval_metric=weighted_zero_mean_r2_lgb,\n            callbacks=[lgb.early_stopping(stopping_rounds=100)])\n        \n        else:\n            print(f'============== {model_name} with Fold {fold}/{N_FOLDS} =========')\n            # create NEW model object for THIS fold\n            model = model_class()\n            model.fit(\n            X_train,\n            y_train,\n            sample_weight=w_train,\n            eval_set=[(X_valid, y_valid)],\n            sample_weight_eval_set=[w_valid],\n            verbose=20,\n            callbacks=[EarlyStopping(rounds=100, maximize=True, save_best=True)],)\n            print(f\"Best iteration: {model.best_iteration}\")\n            print(f\"Best CV score: {model.best_score}\")\n\n        \n        # Save model\n        joblib.dump(model, f\"models/{model_name}_{fold}.model\")\n        models.append((model_name, fold, model))\n        \n        # Delete model to free memory\n        del model\n        gc.collect()\n\n    del fold_df, X_train, y_train, w_train\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T05:51:40.666033Z","iopub.execute_input":"2025-12-14T05:51:40.666295Z","iopub.status.idle":"2025-12-14T06:06:05.020838Z","shell.execute_reply.started":"2025-12-14T05:51:40.666271Z","shell.execute_reply":"2025-12-14T06:06:05.020168Z"}},"outputs":[{"name":"stdout","text":"--------------------- Fold 1/2 -------------------\nTrain dates: from day 1200 to 1399 (200 days)\n--------------------------------------------------\n============== LightGBM with Fold 1/2 =========\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9913\n[LightGBM] [Info] Number of data points in the train set: 7389712, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (563.79 MB) transferred to GPU in 0.545834 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 0.006386\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[519]\tvalid_0's l2: 1.54187\tvalid_0's weighted_zero_mean_r2: 0.00863884\n============== XGBoost with Fold 1/2 =========\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00010\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00191\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00329\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00438\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00523\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00587\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00633\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00673\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00703\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00730\n[200]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00749\n[220]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00763\n[240]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00773\n[260]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00783\n[280]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00788\n[300]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00798\n[320]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00794\n[340]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00792\n[360]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00791\n[380]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00795\n[400]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00797\n[401]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00795\nBest iteration: 302\nBest CV score: 0.007994\n--------------------- Fold 2/2 -------------------\nTrain dates: from day 1400 to 1599 (200 days)\n--------------------------------------------------\n============== LightGBM with Fold 2/2 =========\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 9914\n[LightGBM] [Info] Number of data points in the train set: 7313240, number of used features: 80\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 80 dense feature groups (557.96 MB) transferred to GPU in 0.531103 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score -0.002058\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[395]\tvalid_0's l2: 1.5402\tvalid_0's weighted_zero_mean_r2: 0.00955024\n============== XGBoost with Fold 2/2 =========\n[0]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00011\n[20]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00196\n[40]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00338\n[60]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00457\n[80]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00546\n[100]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00621\n[120]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00688\n[140]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00738\n[160]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00778\n[180]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00810\n[200]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00838\n[220]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00862\n[240]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00882\n[260]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00900\n[280]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00913\n[300]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00928\n[320]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00937\n[340]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00945\n[360]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00954\n[380]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00966\n[400]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00974\n[420]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00980\n[440]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00982\n[460]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00984\n[480]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00982\n[500]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00974\n[520]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00976\n[540]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00970\n[545]\tvalidation_0-weighted_zero_mean_r2_xgb:0.00967\nBest iteration: 445\nBest CV score: 0.00985\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ======================\n# Load Pretrained model \n# ======================\n'''\nmodels_path ='/kaggle/input/models/scikitlearn/default/1/models'\nfor file in os.listdir(models_path):\n    if file.endswith(\".model\"):\n        model = joblib.load(os.path.join(models_path, file))\n        models.append(model)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:06:05.021623Z","iopub.execute_input":"2025-12-14T06:06:05.021841Z","iopub.status.idle":"2025-12-14T06:06:05.026790Z","shell.execute_reply.started":"2025-12-14T06:06:05.021816Z","shell.execute_reply":"2025-12-14T06:06:05.026089Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'\\nmodels_path =\\'/kaggle/input/models/scikitlearn/default/1/models\\'\\nfor file in os.listdir(models_path):\\n    if file.endswith(\".model\"):\\n        model = joblib.load(os.path.join(models_path, file))\\n        models.append(model)\\n'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# ========================================\n# Prediction Using the Ensemble of Models\n# ========================================\n\nlags_: pl.DataFrame | None = None\n\ndef predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame:\n    \"\"\"Make a prediction using the ensemble of models.\"\"\"\n    global lags_\n    if lags is not None:\n        lags_ = lags\n\n    # Convert features to NumPy for model prediction\n    feat = test.select(features_cols).to_numpy()\n    \n    # Ensemble prediction (average over all models)\n    pred = np.mean([model.predict(feat) for model in models], axis=0)\n    \n    # Create Polars DataFrame for submission\n    predictions = pl.DataFrame({\n        'row_id': test['row_id'],\n        'responder_6': pred.astype(np.float32)\n    })\n    \n    # Assertions for safety\n    assert isinstance(predictions, pl.DataFrame)\n    assert list(predictions.columns) == ['row_id', 'responder_6']\n    assert len(predictions) == len(test)\n\n    return predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:08:57.854359Z","iopub.execute_input":"2025-12-14T06:08:57.855222Z","iopub.status.idle":"2025-12-14T06:08:57.860623Z","shell.execute_reply.started":"2025-12-14T06:08:57.855196Z","shell.execute_reply":"2025-12-14T06:08:57.859822Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ===============\n# Submission\n# ===============\ninference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    # If the competition is currently running on Kaggle\n    inference_server.serve()\n\nelif os.getenv('KAGGLE_IS_COMPETITION_ACTIVE'):\n    # If the competition is still active\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/jane-street-realtime-market-data-forecasting/test.parquet',\n            '/kaggle/input/jane-street-realtime-market-data-forecasting/lags.parquet',\n        )\n    )\n\nelse:\n    # After the competition has ended\n    test_df = pl.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet')\n    lags_df = pl.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet/date_id=0/part-0.parquet')\n    \n    predictions = predict(test_df, lags_df)\n    print(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T06:09:03.651647Z","iopub.execute_input":"2025-12-14T06:09:03.652337Z","iopub.status.idle":"2025-12-14T06:09:03.710816Z","shell.execute_reply.started":"2025-12-14T06:09:03.652303Z","shell.execute_reply":"2025-12-14T06:09:03.709874Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/803609951.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlags_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet/date_id=0/part-0.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlags_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1269155839.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(test, lags)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Ensemble prediction (average over all models)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Create Polars DataFrame for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1269155839.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Ensemble prediction (average over all models)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Create Polars DataFrame for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'predict'"],"ename":"AttributeError","evalue":"'tuple' object has no attribute 'predict'","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}